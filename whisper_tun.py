import torch
import json
import whisper
import evaluate
import librosa
import numpy as np
from scipy.stats import mode
from datasets import load_dataset
from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperFeatureExtractor, TrainingArguments, Trainer, DataCollatorForSeq2Seq
from jiwer import wer
import os
import random
from datetime import datetime
from huggingface_hub import login

# âœ… Hugging Face ë¡œê·¸ì¸
HF_TOKEN = os.getenv("HF_TOKEN") or input("ğŸ”‘ Hugging Face Access Token ì…ë ¥: ")
login(token=HF_TOKEN)

# âœ… ê²°ê³¼ ì €ì¥ í´ë” ì„¤ì •
SAVE_PATH = "./whisper-ko-finetuned/"
os.makedirs(SAVE_PATH, exist_ok=True)

# âœ… ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì¼ë¶€ ë°ì´í„° ì„ íƒ
dataset = load_dataset("jp1924/KsponSpeech", split="train")
small_dataset = dataset.select(range(10))  # âœ… 10ê°œ ìƒ˜í”Œë§Œ ì„ íƒ (í…ŒìŠ¤íŠ¸ìš©)

# âœ… Pretrained Whisper ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
processor = WhisperProcessor.from_pretrained("openai/whisper-small")
feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")

# âœ… ë°˜ë³µëœ ê°’ ì œê±° (ê°•í™”ëœ í•„í„°ë§ ì ìš©)
def remove_repeated_values(audio_array, window_size=20, threshold_factor=0.02, mode_threshold=0.5, enable_mode_filter=True):
    """
    ë°˜ë³µë˜ëŠ” íŠ¹ì • ê°’ê³¼ ì¼ì • êµ¬ê°„ ë™ì•ˆ ë³€í™”ê°€ ê±°ì˜ ì—†ëŠ” ë¶€ë¶„ì„ ì œê±°.
    """
    if len(audio_array) < window_size:
        return audio_array  # ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ

    # âœ… ìµœë¹ˆê°’(Mode) ê¸°ë°˜ í•„í„°ë§ (ì„ íƒì  ì ìš©)
    if enable_mode_filter:
        most_common_value, count = mode(audio_array, keepdims=False)
        most_common_value = most_common_value[0]
        if count / len(audio_array) >= mode_threshold:  # íŠ¹ì • ê°’ì´ 50% ì´ìƒ ì°¨ì§€í•˜ë©´ ì‚­ì œ
            audio_array = audio_array[audio_array != most_common_value]

    # âœ… Sliding Windowë¡œ í‰ê·  ë³€í™”ëŸ‰ ê³„ì‚°
    diff = np.abs(np.diff(audio_array))
    avg_diff = np.convolve(diff, np.ones(window_size) / window_size, mode='valid')

    # âœ… Adaptive Threshold: ì „ì²´ ë°ì´í„°ì˜ ë³€í™”ëŸ‰ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì ìœ¼ë¡œ ì„¤ì •
    threshold = np.mean(avg_diff) * threshold_factor

    # âœ… íŠ¹ì • ì„ê³„ê°’ ì´í•˜ì˜ ë³€í™”ë§Œ ìˆëŠ” êµ¬ê°„ì„ ì œê±°
    stable_indices = np.where(avg_diff > threshold)[0] + 1  # ë³€í™”ê°€ ìˆëŠ” ì¸ë±ìŠ¤ë§Œ ìœ ì§€
    filtered_audio = audio_array[stable_indices] if len(stable_indices) > 0 else audio_array

    return np.array(filtered_audio, dtype="float32")

# âœ… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def preprocess_function(batch):
    audio_array = batch["audio"]["array"].astype("float32")
    orig_sr = batch["audio"]["sampling_rate"]

    # âœ… ìƒ˜í”Œë§ ë ˆì´íŠ¸ ê°•ì œ ë³€í™˜ (16kHzë¡œ ë³€í™˜)
    if orig_sr != 16000:
        audio_array = librosa.resample(audio_array, orig_sr=orig_sr, target_sr=16000)

    # âœ… ë¬´ìŒ ì œê±° (librosa.effects.trim ì‚¬ìš©) â†’ ë™ì  ê°ë„ ì¡°ì ˆ
    audio_array, _ = librosa.effects.trim(audio_array, top_db=np.clip(np.std(audio_array) * 10, 15, 30))  

    # âœ… ì—°ì†ëœ ê°’ ì œê±° (ìµœë¹ˆê°’ + Sliding Window ë°©ì‹ ì ìš©)
    audio_array = remove_repeated_values(audio_array, window_size=20, threshold_factor=0.02, mode_threshold=0.5)

    # âœ… ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤ëŠ” í•„í„°ë§
    min_length = 1 * 16000  # ìµœì†Œ 1ì´ˆ ì´ìƒ ìœ ì§€
    if len(audio_array) < min_length:
        return None  # Noneì„ ë°˜í™˜í•˜ë©´ ìë™ìœ¼ë¡œ í•„í„°ë§ë¨

    # âœ… ë„ˆë¬´ ê¸´ ì˜¤ë””ì˜¤ëŠ” ëœë¤ ìƒ˜í”Œë§í•˜ì—¬ 30ì´ˆë¡œ ì œí•œ
    max_length = 30 * 16000  # 30ì´ˆ * 16000Hz
    if len(audio_array) > max_length:
        start_idx = random.randint(0, len(audio_array) - max_length)
        audio_array = audio_array[start_idx:start_idx + max_length]

    # âœ… Feature Extractor ì ìš©
    processed = feature_extractor(audio_array, sampling_rate=16000)
    batch["input_values"] = processed["input_features"][0]

    # âœ… labels ì²˜ë¦¬
    labels = processor.tokenizer(batch["sentence"], truncation=True, padding="longest").input_ids
    if not labels:
        labels = [processor.tokenizer.pad_token_id]  # ë¹ˆ ê²½ìš° íŒ¨ë”© í† í° ì¶”ê°€

    batch["labels"] = labels
    return batch

# âœ… ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì ìš© (None ê°’ ì œê±° í¬í•¨)
small_dataset = small_dataset.map(preprocess_function, remove_columns=["audio", "sentence"])
small_dataset = small_dataset.filter(lambda x: x is not None)  # None ê°’ ì œê±°

# âœ… ë³€í™˜ëœ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…)
print("âœ… ë°ì´í„°ì…‹ ìƒ˜í”Œ í™•ì¸")
print(small_dataset[0])  # ì²« ë²ˆì§¸ ë°ì´í„° í™•ì¸
print("ğŸ§ ë³€í™˜ëœ ì˜¤ë””ì˜¤ ê¸¸ì´:", len(small_dataset[0]["input_values"]))

# âœ… ì´ì œ ë¶ˆí•„ìš”í•œ ë°˜ë³µëœ ê°’ì´ í™•ì‹¤íˆ ì œê±°ë  ê²ƒì…ë‹ˆë‹¤! ğŸš€
