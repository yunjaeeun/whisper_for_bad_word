import torch
import json
import whisper
import evaluate
import librosa
import numpy as np
from scipy.stats import mode
from datasets import load_dataset
from transformers import (
    WhisperForConditionalGeneration, WhisperProcessor, WhisperFeatureExtractor,
    TrainingArguments, Trainer, DataCollatorForSeq2Seq
)
from jiwer import wer
import os
import random
from datetime import datetime
from huggingface_hub import login

# âœ… Hugging Face ë¡œê·¸ì¸
HF_TOKEN = os.getenv("HF_TOKEN") or input("ğŸ”‘ Hugging Face Access Token ì…ë ¥: ")
login(token=HF_TOKEN)

# âœ… ê²°ê³¼ ì €ì¥ í´ë” ì„¤ì •
SAVE_PATH = "./whisper-ko-finetuned/"
os.makedirs(SAVE_PATH, exist_ok=True)

# âœ… ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì¼ë¶€ ë°ì´í„° ì„ íƒ
dataset = load_dataset("jp1924/KsponSpeech", split="train")

# âš ï¸ ì „ì²˜ë¦¬ ì „ì— ì›ë³¸ ì˜¤ë””ì˜¤ ê¸¸ì´ê°€ ìµœì†Œ 1ì´ˆ(16000 ìƒ˜í”Œ) ì´ìƒì¸ ìƒ˜í”Œë§Œ ë‚¨ê¹ë‹ˆë‹¤.
min_length = 1 * 16000
dataset = dataset.filter(lambda x: len(x["audio"]["array"]) >= min_length)

# í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ê°œ ìƒ˜í”Œë§Œ ì„ íƒ (ì›í•˜ëŠ” ê°œìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥)
small_dataset = dataset.select(range(10))

# âœ… Pretrained Whisper ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
processor = WhisperProcessor.from_pretrained("openai/whisper-small")
feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")

# --- ìš”ì•½ í†µê³„ ì •ë³´ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ ---
def summarize_audio(audio_array, num_samples=5):
    """ì˜¤ë””ì˜¤ ë°°ì—´ì— ëŒ€í•œ ìš”ì•½ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    summary = {
        'length': len(audio_array),
        'mean': float(np.mean(audio_array)),
        'std': float(np.std(audio_array)),
        'min': float(np.min(audio_array)),
        'max': float(np.max(audio_array)),
        'head': audio_array[:num_samples].tolist(),
        'tail': audio_array[-num_samples:].tolist()
    }
    return summary

# âœ… ë°˜ë³µëœ ê°’ ì œê±° (ê°•í™”ëœ í•„í„°ë§ ì ìš©)
def remove_repeated_values(audio_array, window_size=20, threshold_factor=0.02, mode_threshold=0.5, enable_mode_filter=True):
    """
    ë°˜ë³µë˜ëŠ” íŠ¹ì • ê°’ê³¼ ì¼ì • êµ¬ê°„ ë™ì•ˆ ë³€í™”ê°€ ê±°ì˜ ì—†ëŠ” ë¶€ë¶„ì„ ì œê±°í•©ë‹ˆë‹¤.
    ê¸°ì¡´ ë°©ì‹ì—ì„œëŠ” ë¬´ìŒ(0 ê·¼ì²˜ì˜ ê°’)ë„ ì œê±°ë  ìˆ˜ ìˆëŠ” ë¬¸ì œê°€ ìˆì—ˆìœ¼ë¯€ë¡œ,
    ì´ë¥¼ ê°œì„ í•˜ì—¬ ë¬´ìŒ ë¶€ë¶„ì€ ìœ ì§€í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    """
    if len(audio_array) < window_size:
        return audio_array  # ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ

    # âœ… ëª¨ë“œ(ìµœë¹ˆê°’) ê¸°ë°˜ í•„í„°ë§ (ë¬´ìŒì— í•´ë‹¹í•˜ëŠ” ê°’ì€ ì œê±°í•˜ì§€ ì•ŠìŒ)
    if enable_mode_filter:
        mode_result = mode(audio_array, keepdims=False)
        most_common_value, count = mode_result.mode, mode_result.count
        # ë§Œì•½ ìµœë¹ˆê°’ì´ numpy ë°°ì—´ì´ë©´ ì²«ë²ˆì§¸ ìš”ì†Œë¥¼ ê°€ì ¸ì˜´, ì•„ë‹ˆë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if not np.isscalar(most_common_value):
            most_common_value = most_common_value[0]
            count = count[0]
        # ìµœë¹ˆê°’ì´ 0(ë˜ëŠ” 0ì— ë§¤ìš° ê°€ê¹Œìš´ ê°’)ì´ ì•„ë‹ˆë¼ë©´ ì œê±° ì§„í–‰
        if not np.isclose(most_common_value, 0, atol=1e-3) and (count / len(audio_array)) >= mode_threshold:
            audio_array = audio_array[audio_array != most_common_value]

    # âœ… Sliding Windowë¡œ ë³€í™”ëŸ‰ ê³„ì‚°
    diff = np.abs(np.diff(audio_array))
    avg_diff = np.convolve(diff, np.ones(window_size) / window_size, mode='valid')
    # âœ… Adaptive Threshold: ì „ì²´ ë°ì´í„°ì˜ ë³€í™”ëŸ‰ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì ìœ¼ë¡œ ì„¤ì •
    threshold = np.mean(avg_diff) * threshold_factor

    # âœ… ë³€í™”ëŸ‰ì´ ì„ê³„ì¹˜ ì´í•˜ì¸ êµ¬ê°„ì€ ì•ˆì •ì ì´ë¼ê³  íŒë‹¨í•˜ì—¬ í•´ë‹¹ ì¸ë±ìŠ¤ëŠ” ì œê±°
    # (+1: diff ì—°ì‚°ìœ¼ë¡œ ê¸¸ì´ê°€ 1 ì¤„ì–´ë“¦)
    stable_indices = np.where(avg_diff > threshold)[0] + 1  
    filtered_audio = audio_array[stable_indices] if len(stable_indices) > 0 else audio_array

    return np.array(filtered_audio, dtype="float32")

# âœ… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ (ìš”ì•½ í†µê³„ ì •ë³´ë¥¼ ì¶œë ¥)
def preprocess_function(batch):
    print(f"=== Processing sample id: {batch['id']} ===")
    
    # ì›ë³¸ ì˜¤ë””ì˜¤ ì •ë³´ ìš”ì•½ ì¶œë ¥
    audio_array = batch["audio"]["array"].astype("float32")
    orig_sr = batch["audio"]["sampling_rate"]
    original_summary = summarize_audio(audio_array)
    print(f"Original audio summary: {original_summary}")
    
    # ìƒ˜í”Œë§ ë ˆì´íŠ¸ ê°•ì œ ë³€í™˜ (16kHzë¡œ ë³€í™˜)
    if orig_sr != 16000:
        audio_array = librosa.resample(audio_array, orig_sr=orig_sr, target_sr=16000)
        print("Resampled audio to 16000 Hz")
    
    # ë¬´ìŒ ì œê±° (librosa.effects.trim ì‚¬ìš©) â†’ ë™ì  ê°ë„ ì¡°ì ˆ
    audio_array, _ = librosa.effects.trim(
        audio_array, top_db=np.clip(np.std(audio_array) * 10, 15, 30)
    )
    print(f"After trim, audio summary: {summarize_audio(audio_array)}")
    
    # ì—°ì†ëœ ê°’ ì œê±° (ìµœë¹ˆê°’ + Sliding Window ë°©ì‹ ì ìš©)
    audio_array = remove_repeated_values(
        audio_array, window_size=20, threshold_factor=0.02, mode_threshold=0.5
    )
    print(f"After remove_repeated_values, audio summary: {summarize_audio(audio_array)}")
    
    # ì²˜ë¦¬ í›„ ê¸¸ì´ê°€ 1ì´ˆ ë¯¸ë§Œì´ë©´ ë“œë¡­ (None ëŒ€ì‹  í”Œë˜ê·¸ ì‚¬ìš©)
    if len(audio_array) < min_length:
        print(f"Sample id {batch['id']} dropped: processed length {len(audio_array)} is below minimum {min_length}")
        return {
            "id": batch["id"],
            "input_values": None,
            "labels": None,
            "drop": True
        }
    
    # ë„ˆë¬´ ê¸´ ì˜¤ë””ì˜¤ëŠ” ëœë¤ ìƒ˜í”Œë§í•˜ì—¬ 30ì´ˆë¡œ ì œí•œ
    max_length = 30 * 16000  # 30ì´ˆ
    if len(audio_array) > max_length:
        start_idx = random.randint(0, len(audio_array) - max_length)
        audio_array = audio_array[start_idx:start_idx + max_length]
        print(f"Trimmed long audio to max_length: {max_length}")
    
    # Feature Extractor ì ìš©
    processed = feature_extractor(audio_array, sampling_rate=16000)
    batch["input_values"] = processed["input_features"][0]
    
    # labels ì²˜ë¦¬  
    # â†’ tokenizer.pad()ê°€ ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•˜ë„ë¡, tokenizerì˜ ì „ì²´ ì¸ì½”ë”©(dict)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    labels_encoding = processor.tokenizer(batch["sentence"], truncation=True, padding="longest")
    if not labels_encoding["input_ids"]:
        labels_encoding["input_ids"] = [processor.tokenizer.pad_token_id]
    batch["labels"] = labels_encoding
    # (ëª¨ë¸ í•™ìŠµ ì‹œ, Trainer ë‚´ë¶€ì—ì„œ labels["input_ids"]ë¥¼ tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤.)
    
    # ë“œë¡­ í”Œë˜ê·¸ False ì„¤ì •
    batch["drop"] = False
    print(f"Sample id {batch['id']} processed successfully.\n")
    return batch

# âœ… ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì ìš© (ì›ë˜ì˜ "audio"ì™€ "sentence" ì»¬ëŸ¼ì€ ì œê±°)
small_dataset = small_dataset.map(preprocess_function, remove_columns=["audio", "sentence"])

# âœ… ë“œë¡­ í”Œë˜ê·¸ê°€ Trueì¸ ìƒ˜í”Œ ì œê±°
small_dataset = small_dataset.filter(lambda x: not x["drop"])
# ì‚¬ìš© í›„ drop í”Œë˜ê·¸ ì»¬ëŸ¼ ì œê±° (í•„ìš” ì‹œ)
small_dataset = small_dataset.remove_columns("drop")

# âœ… ë³€í™˜ëœ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…)
print("âœ… ë°ì´í„°ì…‹ ìƒ˜í”Œ í™•ì¸")
print(small_dataset[0])
print("ğŸ§ ë³€í™˜ëœ ì˜¤ë””ì˜¤ ê¸¸ì´:", len(small_dataset[0]["input_values"]))

# -----------------------------------------------------------------------------
# í•™ìŠµ (fine-tuning) ë¶€ë¶„ ì‹œì‘
# -----------------------------------------------------------------------------

# TrainingArguments ì„¤ì • (í™˜ê²½ì— ë§ê²Œ ì¡°ì •)
training_args = TrainingArguments(
    output_dir=SAVE_PATH,
    per_device_train_batch_size=1,    # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ
    num_train_epochs=3,               # ì—í­ ìˆ˜ ì¡°ì ˆ
    logging_steps=10,
    save_steps=50,
    evaluation_strategy="no",         # FutureWarning: eval_strategy ì‚¬ìš© ê¶Œì¥ (ë‹¹ì¥ì€ ë¬´ì‹œ)
    fp16=False,                       # GPUê°€ FP16ì„ ì§€ì›í•œë‹¤ë©´ Trueë¡œ ë³€ê²½ ê°€ëŠ¥
)

# DataCollator ì„¤ì •: Whisper ëª¨ë¸ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ë¼ë²¨ ëª¨ë‘ íŒ¨ë”©ì´ í•„ìš”í•©ë‹ˆë‹¤.
data_collator = DataCollatorForSeq2Seq(
    tokenizer=processor.tokenizer,    # ìˆ˜ì •: processor.tokenizerë¥¼ ì‚¬ìš©
    model=model,
    padding=True,
)

# Trainer ì„¤ì •: train_datasetìœ¼ë¡œ small_dataset ì‚¬ìš©
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_dataset,
    data_collator=data_collator,
    tokenizer=processor.tokenizer,    # ìˆ˜ì •: processor.tokenizerë¥¼ ì‚¬ìš© (FutureWarning: processing_class ì‚¬ìš© ê¶Œì¥)
)

# ëª¨ë¸ í•™ìŠµ ì‹œì‘
trainer.train()
